{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begins with...\n",
    "- Use the csv file from Kickstarter Datasets, which collect Kickstarter project data each month, as our data sourse. However, there might be duplicate data in Kickstarter Datasets, so filter out duplicate project through project_id.\n",
    "### How we scrape data\n",
    "- First, extract \"project_id\" info and save it in new column. Record project_id in a separete file to filter out duplicate.\n",
    "- Second, extract corrosponding URL and save it into new column.\n",
    "- Third, script web through URL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First part\n",
    "- Extract \"project_id\" info and save it in new column. Record project_id in a separete file to filter out duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start filter data from \"2019-02-14\" folder to older folder until \"2018-02-15\" folder.\n",
    "### (did not show each filter code in between months here)\n",
    "### write getPid(x) function to parse project_id and save into new column.\n",
    "### if project_id appears before, assign \"N\" as duplicate\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "project_id_dict={}\n",
    "\n",
    "# define a get project_id function, and store unique project_id to dictionary\n",
    "def getPid(x):\n",
    "    global project_id_dict\n",
    "    \n",
    "    section=x.split(',')\n",
    "    new_id=section[1].split(':')[1]\n",
    "    \n",
    "    # record unique project_id to dict\n",
    "    if new_id in project_id_dict:\n",
    "        new_id=\"N\"\n",
    "    else:\n",
    "        project_id_dict[new_id]=1\n",
    "        \n",
    "    # for adding new column \"project_id\"\n",
    "    return new_id\n",
    "directory ='C:/Users/USER/Desktop/Kickstarter_2019-02-14T03_20_04_734Z/'\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    #read csv, extract profile column.\n",
    "    df=pd.read_csv(directory+file, header=0)\n",
    "    \n",
    "    # add project id column\n",
    "    df['project_id']=df['profile'].apply(getPid)\n",
    "    \n",
    "    # save df to new folder    \n",
    "    df.to_csv(\"C:/Users/USER/Desktop/2019-02-14/\"+file, header=True, index=True)\n",
    "\n",
    "# save dictionary to file\n",
    "json.dump(project_id_dict, open(\"C:/Users/USER/Desktop/pid.json\",\"w\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter \"2018-02-15\" folder\n",
    "\n",
    "# load dictionary log\n",
    "project_id_dict = json.load(open(\"C:/Users/USER/Desktop/pid11.json\",\"r\"))\n",
    "\n",
    "directory =\"C:/Users/USER/Desktop/Kickstarter_2018-02-15T03_20_44_743Z/\"\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    #read csv, extract profile column. (fix UnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0x8b w/ encoding)\n",
    "    df=pd.read_csv(directory+file, header=0, encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    # add project id column\n",
    "    df['project_id']=df['profile'].apply(getPid)\n",
    "    \n",
    "    # save df to new folder    \n",
    "    df.to_csv(\"C:/Users/USER/Desktop/2018-02-15/\"+file, header=True, index=True)\n",
    "\n",
    "# save dictionary to file\n",
    "json.dump(project_id_dict, open(\"C:/Users/USER/Desktop/pid12.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check length of dictionary for each dictionary\n",
    "- find that the length of dict stop increase after \"pid12.json\", which means that no new project_id after that file. Thus, collected data are from the lattest 2019-02-14 to 2018-03-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "directory =\"C:/Users/USER/Desktop/id_log/\"\n",
    "\n",
    "for i in os.listdir(directory):\n",
    "    # load dictionary log\n",
    "    f = json.load(open(directory+i,\"r\"))\n",
    "    print(\"length of dict:\",i,len(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second part\n",
    "- Extract corrosponding URL and save it into new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add URL column into csv file\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# write a get URL function, could apply to dataframe\n",
    "def getURL(x):\n",
    "    url=x.split('\"')[5]\n",
    "    return url\n",
    "\n",
    "# go through one folder(here start from \"2019-02-14\")\n",
    "directory =\"C:/Users/USER/Desktop/Process_k/2019-02-14/\"\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    # get dataframe\n",
    "    df=pd.read_csv(directory+file, header=0)\n",
    "    \n",
    "    # add URL column, where project_id!=\"N\" (fix Chained assignment problem w/ .loc)\n",
    "    df['url']=df.loc[df.project_id!=\"N\"]['urls'].apply(getURL)\n",
    "    \n",
    "    # save df to new folder    \n",
    "    df.to_csv(\"C:/Users/USER/Desktop/2019-02-14/\"+file, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through 2019-01-17 folder\n",
    "directory =\"C:/Users/USER/Desktop/Process_k/2019-01-17/\"\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    #read csv, extract profile column.\n",
    "    df=pd.read_csv(directory+file, header=0)\n",
    "    \n",
    "    # add URL column, where project_id!=\"N\" (fix Chained assignment problem)\n",
    "    df['url']=df.loc[df.project_id!=\"N\"]['urls'].apply(getURL)\n",
    "    \n",
    "    # save df to new folder    \n",
    "    df.to_csv(\"C:/Users/USER/Desktop/2019-01-17/\"+file, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through 2018-12-13 folder\n",
    "directory =\"C:/Users/USER/Desktop/Process_k/2018-12-13/\"\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    #read csv, extract profile column.\n",
    "    df=pd.read_csv(directory+file, header=0)\n",
    "    \n",
    "    # add URL column, where project_id!=\"N\" (fix Chained assignment problem)\n",
    "    df['url']=df.loc[df.project_id!=\"N\"]['urls'].apply(getURL)\n",
    "    \n",
    "    # save df to new folder    \n",
    "    df.to_csv(\"C:/Users/USER/Desktop/2018-12-13/\"+file, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through 2018-11-15 folder\n",
    "directory =\"C:/Users/USER/Desktop/Process_k/2018-11-15/\"\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    #read csv, extract profile column.\n",
    "    df=pd.read_csv(directory+file, header=0)\n",
    "    \n",
    "    # add URL column, where project_id!=\"N\" (fix Chained assignment problem)\n",
    "    df['url']=df.loc[df.project_id!=\"N\"]['urls'].apply(getURL)\n",
    "    \n",
    "    # save df to new folder    \n",
    "    df.to_csv(\"C:/Users/USER/Desktop/2018-11-15/\"+file, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through 2018-10-18 folder\n",
    "directory =\"C:/Users/USER/Desktop/Process_k/2018-10-18/\"\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    #read csv, extract profile column.\n",
    "    df=pd.read_csv(directory+file, header=0)\n",
    "    \n",
    "    # add URL column, where project_id!=\"N\" (fix Chained assignment problem)\n",
    "    df['url']=df.loc[df.project_id!=\"N\"]['urls'].apply(getURL)\n",
    "    \n",
    "    # save df to new folder    \n",
    "    df.to_csv(\"C:/Users/USER/Desktop/2018-10-18/\"+file, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through 2018-09-13 folder\n",
    "directory =\"C:/Users/USER/Desktop/Process_k/2018-09-13/\"\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    #read csv, extract profile column.\n",
    "    df=pd.read_csv(directory+file, header=0)\n",
    "    \n",
    "    # add URL column, where project_id!=\"N\" (fix Chained assignment problem)\n",
    "    df['url']=df.loc[df.project_id!=\"N\"]['urls'].apply(getURL)\n",
    "    \n",
    "    # save df to new folder    \n",
    "    df.to_csv(\"C:/Users/USER/Desktop/2018-09-13/\"+file, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through 2018-08-16 folder\n",
    "directory =\"C:/Users/USER/Desktop/Process_k/2018-08-16/\"\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    #read csv, extract profile column.\n",
    "    df=pd.read_csv(directory+file, header=0)\n",
    "    \n",
    "    # add URL column, where project_id!=\"N\" (fix Chained assignment problem)\n",
    "    df['url']=df.loc[df.project_id!=\"N\"]['urls'].apply(getURL)\n",
    "    \n",
    "    # save df to new folder    \n",
    "    df.to_csv(\"C:/Users/USER/Desktop/2018-08-16/\"+file, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through 2018-07-12 folder\n",
    "directory =\"C:/Users/USER/Desktop/Process_k/2018-07-12/\"\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    #read csv, extract profile column.\n",
    "    df=pd.read_csv(directory+file, header=0)\n",
    "    \n",
    "    # add URL column, where project_id!=\"N\" (fix Chained assignment problem)\n",
    "    df['url']=df.loc[df.project_id!=\"N\"]['urls'].apply(getURL)\n",
    "    \n",
    "    # save df to new folder    \n",
    "    df.to_csv(\"C:/Users/USER/Desktop/2018-07-12/\"+file, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through 2018-06-14 folder\n",
    "directory =\"C:/Users/USER/Desktop/Process_k/2018-06-14/\"\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    #read csv, extract profile column.\n",
    "    df=pd.read_csv(directory+file, header=0)\n",
    "    \n",
    "    # add URL column, where project_id!=\"N\" (fix Chained assignment problem)\n",
    "    df['url']=df.loc[df.project_id!=\"N\"]['urls'].apply(getURL)\n",
    "    \n",
    "    # save df to new folder    \n",
    "    df.to_csv(\"C:/Users/USER/Desktop/2018-06-14/\"+file, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through 2018-05-17 folder\n",
    "directory =\"C:/Users/USER/Desktop/Process_k/2018-05-17/\"\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    #read csv, extract profile column.\n",
    "    df=pd.read_csv(directory+file, header=0)\n",
    "    \n",
    "    # add URL column, where project_id!=\"N\" (fix Chained assignment problem)\n",
    "    df['url']=df.loc[df.project_id!=\"N\"]['urls'].apply(getURL)\n",
    "    \n",
    "    # save df to new folder    \n",
    "    df.to_csv(\"C:/Users/USER/Desktop/2018-05-17/\"+file, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through 2018-04-12 folder\n",
    "directory =\"C:/Users/USER/Desktop/Process_k/2018-04-12/\"\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    #read csv, extract profile column.\n",
    "    df=pd.read_csv(directory+file, header=0)\n",
    "    \n",
    "    # add URL column, where project_id!=\"N\" (fix Chained assignment problem)\n",
    "    df['url']=df.loc[df.project_id!=\"N\"]['urls'].apply(getURL)\n",
    "    \n",
    "    # save df to new folder    \n",
    "    df.to_csv(\"C:/Users/USER/Desktop/2018-04-12/\"+file, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through 2018-03-15 folder\n",
    "directory =\"C:/Users/USER/Desktop/Process_k/2018-03-15/\"\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    #read csv, extract profile column.\n",
    "    df=pd.read_csv(directory+file, header=0)\n",
    "    \n",
    "    # add URL column, where project_id!=\"N\" (fix Chained assignment problem)\n",
    "    df['url']=df.loc[df.project_id!=\"N\"]['urls'].apply(getURL)\n",
    "    \n",
    "    # save df to new folder    \n",
    "    df.to_csv(\"C:/Users/USER/Desktop/2018-03-15/\"+file, header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third part\n",
    " - script web through URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### trial and error part\n",
    "### method 1: scraping website throgh selenium with scroll down.(this method takes time, so consider another method)\n",
    "### scrapt website with assigned URL\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Path where you save the webdriver (whole path to \"geckodriver.exe\")\n",
    "executable_path = 'C:/Users/USER/Desktop/course/BIA 660 Web Mining/geckodriver-v0.24.0-win64/geckodriver.exe'\n",
    "\n",
    "# initiator the webdriver for Firefox browser\n",
    "driver = webdriver.Firefox(executable_path=executable_path)\n",
    "\n",
    "# send a request\n",
    "driver.get('https://www.kickstarter.com/projects/704877813/wellmii-your-healthjust-perfectly-organised?ref=category_newest')\n",
    "\n",
    "src_updated = driver.page_source\n",
    "src = \"\"\n",
    "\n",
    "while src != src_updated:\n",
    "    \n",
    "    # save page source (i.e. html document) before page-down\n",
    "    src = src_updated\n",
    "    \n",
    "    # execute javascript to scroll to the bottom of the window\n",
    "    # you can also use page-down\n",
    "    driver.execute_script(\"window.scrollTo(0, \\\n",
    "    document.body.scrollHeight);\")\n",
    "    \n",
    "    # sleep to allow content loaded\n",
    "    time.sleep(.5)\n",
    "    \n",
    "    # save page source after page-down\n",
    "    src_updated = driver.page_source\n",
    "\n",
    "# get reward section\n",
    "rewards=driver.find_elements_by_css_selector(\"div.js-project-rewards ol li\")\n",
    "reward_list=[]\n",
    "for i, r in enumerate(rewards):\n",
    "    print(i, r.text)\n",
    "    reward_list.append(r.text)\n",
    "print(reward_list)\n",
    "print(\"\\n\")\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### method 2: scraping website throgh BeautifulSoup.(this method save a lot of time, so adopt this method)\n",
    "### scrapt website with assigned URL\n",
    "\n",
    "# import requests package\n",
    "import requests                   \n",
    "\n",
    "# import BeautifulSoup from package bs4 (i.e. beautifulsoup4)\n",
    "from bs4 import BeautifulSoup     \n",
    "\n",
    "page = requests.get(\"https://www.kickstarter.com/projects/704877813/wellmii-your-healthjust-perfectly-organised?ref=category_newest\") \n",
    "dictionary={}\n",
    "\n",
    "# status_code 200 indicates success. \n",
    "#a status code >200 indicates a failure \n",
    "if page.status_code==200:        \n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    ## Separator in your get_text, which will fetch all the strings \n",
    "    ## in the current element separated by the given character.\n",
    "    # get title\n",
    "    title=soup.select(\"div.col-20-24\")\n",
    "    print('title:', title[0].get_text(separator=\"\\n\", strip=True))   \n",
    "    print('\\n')\n",
    "\n",
    "    # get progress\n",
    "    progress=soup.select(\"div.col-md-8-24\")\n",
    "    print('Progress:', progress[0].get_text(separator=\"\\n\", strip=True)) \n",
    "    print('\\n')\n",
    "\n",
    "    # get all description \n",
    "    desc=soup.select(\"div.full-description\")\n",
    "    print(\"Desciption: \",desc[0].get_text(separator=\"\\n\", strip=True))\n",
    "    print('\\n')\n",
    "\n",
    "    # get risk section\n",
    "    risk=soup.select(\"div.js-risks\")\n",
    "    print(\"Risk: \",risk[0].get_text(separator=\"\\n\", strip=True))\n",
    "    print('******')\n",
    "\n",
    "    # get reward section\n",
    "    rewards=soup.select(\"div.js-project-rewards ol li\")\n",
    "    for i, r in enumerate(rewards):\n",
    "        print(\"***\",i, r.get_text(separator=\"\\n\", strip=True))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # get creater name\n",
    "    creater=soup.select(\"div.items-start-md\")\n",
    "    print('creater:', creater[0].get_text(separator=\"\\n\", strip=True))\n",
    "    print('\\n')\n",
    "\n",
    "    # get category and location info\n",
    "    cat_n_loc=soup.select(\"div.col-24-16\")\n",
    "    print('category and location:',cat_n_loc[0].get_text(separator=\"\\n\", strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write a function to get all scripts from assigned URL, applying scraping method 2 above\n",
    "def getScript(url):\n",
    "    page = requests.get(url) \n",
    "    d={}\n",
    "\n",
    "    if page.status_code==200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "        ## (\"Separator\" in get_text will fetch all the strings \n",
    "        ## in the current element separated by the given character.)\n",
    "        # get title\n",
    "        try:\n",
    "            title=soup.select(\"div.col-20-24\")\n",
    "            d['title']=title[0].get_text(separator=\" \", strip=True)\n",
    "        except:\n",
    "            d['title']=None\n",
    "\n",
    "        # get progress\n",
    "        try:\n",
    "            progress=soup.select(\"div.col-md-8-24\")\n",
    "            d['progress']=progress[0].get_text(separator=\" \", strip=True)\n",
    "        except:\n",
    "            d['progress']=None\n",
    "\n",
    "        # get all description\n",
    "        try:\n",
    "            desc=soup.select(\"div.full-description\")\n",
    "            d[\"descripton\"]=desc[0].get_text(separator=\" \", strip=True)\n",
    "        except:\n",
    "            d['descripton']=None\n",
    "\n",
    "        # get risk section\n",
    "        try:\n",
    "            risk=soup.select(\"div.js-risks\")\n",
    "            d[\"risk\"]=risk[0].get_text(separator=\" \", strip=True)\n",
    "        except:\n",
    "            d['risk']=None\n",
    "\n",
    "        # get reward section\n",
    "        try:\n",
    "            rewards=soup.select(\"div.js-project-rewards ol li\")\n",
    "            ls=[]\n",
    "            for i, r in enumerate(rewards):\n",
    "                ls.append(r.get_text(separator=\" \", strip=True))\n",
    "            d[\"reward\"]=ls\n",
    "        except:\n",
    "            d['reward']=None\n",
    "            \n",
    "        # get creater name\n",
    "        try:\n",
    "            create_by=soup.select(\"div.items-start-md\")\n",
    "            d['create_by']=create_by[0].get_text(separator=\" \", strip=True)\n",
    "        except:\n",
    "            d['create_by']=None\n",
    "        \n",
    "        # get category and location info\n",
    "        try:\n",
    "            cat_n_loc=soup.select(\"div.col-24-16\")\n",
    "            d['category and location']=cat_n_loc[0].get_text(separator=\" \", strip=True)\n",
    "            \n",
    "        except:\n",
    "            d['category and location']=None   \n",
    "                \n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get all script and save them seperately to each column\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import request\n",
    "from bs4 import BeautifulSoup     \n",
    "\n",
    "# go through one folder\n",
    "# (same process from 2019-02-14 folder to 2018-03-15 folder, here is the code through 2018-03-15 folder only)\n",
    "directory =\"C:/Users/USER/Desktop/2018-03-15/\"\n",
    "\n",
    "#for file in os.listdir(directory):\n",
    "    df=pd.read_csv(directory+file, header=0)\n",
    "    \n",
    "    # add script_dict column, where url!=None (fix Chained assignment problem w/ .loc)\n",
    "    df['script_dict']=df.loc[df['url'].notnull()]['url'].apply(getScript)\n",
    "    \n",
    "    # save info seperately to each column\n",
    "    df[\"title\"]=df.loc[(df['script_dict'].notnull()) & (df['script_dict']!={}) ]['script_dict'].\\\n",
    "    apply(lambda x: x[\"title\"])\n",
    "    \n",
    "    df['progress']=df.loc[(df['script_dict'].notnull()) & (df['script_dict']!={}) ]['script_dict'].\\\n",
    "    apply(lambda x: x['progress'])\n",
    "    \n",
    "    df[\"descripton\"]=df.loc[(df['script_dict'].notnull()) & (df['script_dict']!={}) ]['script_dict'].\\\n",
    "    apply(lambda x: x[\"descripton\"])\n",
    "    \n",
    "    df[\"risk\"]=df.loc[(df['script_dict'].notnull()) & (df['script_dict']!={}) ]['script_dict'].\\\n",
    "    apply(lambda x: x[\"risk\"])\n",
    "    \n",
    "    df[\"reward\"]=df.loc[(df['script_dict'].notnull()) & (df['script_dict']!={}) ]['script_dict'].\\\n",
    "    apply(lambda x: x[\"reward\"])\n",
    "    \n",
    "    df[\"create_by\"]=df.loc[(df['script_dict'].notnull()) & (df['script_dict']!={}) ]['script_dict'].\\\n",
    "    apply(lambda x: x[\"create_by\"])\n",
    "    \n",
    "    df[\"category and location\"]=df.loc[(df['script_dict'].notnull()) & (df['script_dict']!={}) ]['script_dict'].\\\n",
    "    apply(lambda x: x[\"category and location\"])\n",
    "   \n",
    "    # save df to new folder    \n",
    "    df.to_csv(\"C:/Users/USER/Desktop/t_out/\"+file, header=True, index=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
